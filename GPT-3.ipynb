{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPython will make a temporary file named: C:\\Users\\yhu42\\AppData\\Local\\Temp\\ipython_edit_vfhz_ifa\\ipython_edit_rzpz52fn.py\n"
     ]
    }
   ],
   "source": [
    "# %edit\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.1-py2.py3-none-any.whl (21 kB)\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-3 code was run in April, 2023\n",
    "import openai\n",
    "# import jax.numpy as jnp\n",
    "# from jax.scipy.special import logsumexp\n",
    "from typing import Optional, Tuple, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "# import hydra\n",
    "# from cdt.causality.pairwise import RECI\n",
    "import os\n",
    "from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n",
    "import re\n",
    "from diskcache import Cache\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "from transformers import pipeline, set_seed\n",
    "import random\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = Cache(\"./\")\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache.memoize()\n",
    "def get_answer_logits(prompt, n_tokens = 16, temp=1.0, logprobs = 5, engine = \"text-davinci-002\"):\n",
    "    \"\"\"Gets logits for the possible next tokens\n",
    "    Args:\n",
    "        prompt: prompt for GPT-3\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "            engine=engine,\n",
    "            prompt=prompt,\n",
    "            max_tokens =  n_tokens,\n",
    "            temperature=temp,\n",
    "            top_p=1,\n",
    "            n=1,\n",
    "            logprobs=logprobs,  # 100 is the maximum here,\n",
    "            presence_penalty=0,\n",
    "            frequency_penalty=0,\n",
    "            best_of=1,\n",
    "            echo=True,\n",
    "        )\n",
    "\n",
    "    text_generate = response['choices'][0]['text'].replace(prompt, \"\")\n",
    "    logprobs_generate = response['choices'][0]['logprobs']\n",
    "    # tokenized prompt\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    # tokenized answer\n",
    "    text_generate_tokes = logprobs_generate['tokens'][len(prompt_tokens):]\n",
    "    # logprob of each answer token\n",
    "    text_generate_logprobs = logprobs_generate['top_logprobs'][len(prompt_tokens):]\n",
    "\n",
    "    return {\n",
    "        'answer': text_generate,\n",
    "        'answer_tokens': text_generate_tokes,\n",
    "        'answer_logprobs': text_generate_logprobs\n",
    "    }\n",
    "\n",
    "@cache.memoize()\n",
    "def get_place_recognition_answer_logits(row, recog_prompt, tokenizer, \n",
    "                                    text_key = \"text\",\n",
    "                                    engine = \"text-davinci-002\", \n",
    "                                    n_tokens = 16, \n",
    "                                    temp=1.0, \n",
    "                                    logprobs = 5):\n",
    "    \n",
    "    text = row[text_key]\n",
    "    # phrase = row[\"phrase\"]\n",
    "    # dbpediaID = row[\"dbpediaID\"]\n",
    "    # ID = row[\"ID\"]\n",
    "    \n",
    "    text_ = text.capitalize()\n",
    "\n",
    "    recog_prompt_ = recog_prompt.replace(\"{TEXT}\", text_)\n",
    "    \n",
    "\n",
    "    # n_answer_tokens = len(tokenizer.encode(phrase)) + 1\n",
    "\n",
    "    res_dict = get_answer_logits(prompt = recog_prompt_, \n",
    "                          n_tokens = n_tokens, \n",
    "                          temp=temp, \n",
    "                          logprobs = logprobs, \n",
    "                          engine = engine)\n",
    "    res_dict = {**res_dict,\n",
    "                **dict(row)\n",
    "               }\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 22 examples\n",
    "recog_prompt = \"\"\"This is a set of location description recognition problems.\n",
    "The `Sentence` is a sentence containing location descriptions.\n",
    "The goal is to infer which parts of the sentence represent location descriptions and the categories of the location descriptions. Split different location descriptions with `;`.\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Papa stranded in home. Water rising above waist. HELP 812 Wood Ln, 77828 #houstonflood\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C1: 812 Wood Ln, 77828 \n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Anyone doing high water rescues in the Pasadena/Deer Park area? My daughter has been stranded in a parking lot all night\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C10: Pasadena/Deer Park\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Allen Parkway, Memorial, Waugh overpass, Spotts park and Buffalo Bayou park completely under water\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C2: Allen Parkway; C2: Memorial; C2: Waugh overpass; C7: Spotts park; C7: Buffalo Bayou park\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Streets Flooded: Almeda Genoa Rd. from Windmill Lakes Blvd. to Rowlett Rd. HurricaneHarvey Houston\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C11: Almeda Genoa Rd. from Windmill Lakes Blvd. to Rowlett Rd.; C9: Houston\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Cleaning supply drive is underway. 9-11 am today at Preston Hollow Presbyterian Church\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C8: Preston Hollow Presbyterian Church\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: #Harvey LIVE from San Antonio, TX. Fatal car accident at Ingram Rd., Strong winds.\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C9: San Antonio; C9: TX; C2: Ingram Rd.\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: 9:00AM update video from Hogan St over White Oak Bayou, I-10, I-45: water down about 4’ since last night.\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C5: Hogan St over White Oak Bayou; C3: I-10; C3: I-45\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Left Corpus bout to be in San Angelo #HurricaneHarvey Y’all be safe Avoided highway 37 Took the back road \n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C9: Corpus; C9: San Angelo; C3: highway 37\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Need trailers/trucks to move dogs from Park Location: Whites Park Pavillion off I-10 exit 61 Anahuac TX\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C7: Whites Park Pavillion; C3: I-10; C4: exit 61; C9: Anahuac; C9: TX\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Townsend exit, Sorters road and Hamblen road is flooded coming from 59 southbound HurricaneHarvery Harvey2017 \n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C4: Townsend exit; C5: Sorters road and Hamblen road; C3: 59 southbound\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: #Harvey does anyone know about the flooding conditions around Cypress Ridge High School?! #HurricaneHarvey\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C8: Cypress Ridge High School\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: FYI to any of you in NW Houston/Lakewood Forest, Projections are showing Cypress Creek overflowing at Grant Rd\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C10: NW Houston/Lakewood Forest; C5: Cypress Creek overflowing at Grant Rd\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: #HurricaneHarvey family needs rescuing at 11800 Grant Rd. Apt. 1009 in Cypress, Texas 77429, 2 elderly, one is 90 not steady in her feet\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C1: 11800 Grant Rd. Apt. 1009 in Cypress, Texas 77429\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Guys, this is I-45 @ Main Street in Houston. Crazy. #hurricane #harvey. . .\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C5: I-45 @ Main Street; C9: Houston\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Frontage Rd at the river #hurricaneHarvey #hurricaneharvey @ San Jacinto River\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C2: Frontage Rd; C6: San Jacinto River\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Pictures of downed trees and damaged apartment building on Airline Road in Corpus Christi.\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C2: Airline Road; C9: Corpus Christi\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Buffalo Bayou holding steady at 10,000 cfs at the gage near Terry Hershey Park\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C6: Buffalo Bayou; C7: Terry Hershey Park\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Major flooding at Clay Rd & Queenston in west Houston. Lots of rescues going on for ppl trapped...\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C5: Clay Rd & Queenston; C9: Houston\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: HELP! A pregnant lady is stuck in her car on I-45 between Cypress Hill & Huffmeister exits #harvey\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C11: I-45 between Cypress Hill & Huffmeister exits\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: If you need a place to escape #HurricaneHarvey, The Willie De Leon Civic Center: 300 E. Main St in Uvalde is open as a shelter\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C7: The Willie De Leon Civic Center; C1: 300 E. Main St in Uvalde\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: Houston’s Buffalo Bayou Park - always among the first to flood. #Harvey\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C9: Houston; C7: Buffalo Bayou Park\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: #HurricaneHarvey INTENSE eye wall of category 4 Hurricane Harvey from Rockport, TX\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A: C9: Rockport; C9: TX\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: {TEXT}\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1510"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(recog_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_HarveyTweet2017_data(anno_file, data_dir, sep = \"||\"):\n",
    "    with open(anno_file, 'r') as f:\n",
    "        i = 0\n",
    "        data_list = []\n",
    "        for line in f:\n",
    "            line = line.split(\"\\n\")[0]\n",
    "            parts = line.split(sep)\n",
    "            # print(parts)\n",
    "            if i == 0:\n",
    "                cols = parts\n",
    "            else:\n",
    "                file = os.path.join(data_dir, parts[0])\n",
    "                text = parts[1]\n",
    "                anno = json.loads(parts[2])\n",
    "                item = {\n",
    "                    \"ID\": i - 1,\n",
    "                    cols[0]: file,\n",
    "                    cols[1]: text,\n",
    "                    cols[2]: anno,\n",
    "                }\n",
    "\n",
    "                data_list.append(item)\n",
    "\n",
    "            i += 1\n",
    "    return data_list\n",
    "\n",
    "def read_res_file(res_file):\n",
    "    res_dict_list = []\n",
    "    \n",
    "    if os.path.exists(res_file):\n",
    "        with open(res_file, 'r') as f:\n",
    "            for line in f:\n",
    "                res_dict = json.loads(line)\n",
    "                res_dict_list.append(res_dict)\n",
    "    return res_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = \"text-davinci-002\"  \n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "data_dir = \"data/\"\n",
    "anno_file = os.path.join(data_dir, 'tweetTextAndAnnotation.txt')\n",
    "data_list = load_HarveyTweet2017_data(anno_file, data_dir, sep = \"||\")\n",
    "\n",
    "\n",
    "\n",
    "n_tokens = 30\n",
    "temp=0.2\n",
    "logprobs = 5\n",
    "logprobs_topk = 20\n",
    "\n",
    "res_dir = \"result/\"  #random22_outside_1000\n",
    "if not os.path.exists(res_dir):\n",
    "    os.mkdir(res_dir)\n",
    "res_file = \"{res_dir}/placerecognize_{engine}_{temp:.3f}_{n_tokens:d}_{logprobs:d}_22examples.json\".format(\n",
    "        res_dir = res_dir,\n",
    "        engine = engine,\n",
    "        temp = temp,\n",
    "        n_tokens = n_tokens,\n",
    "        logprobs = logprobs\n",
    "    )\n",
    "res_dict_list = read_res_file(res_file)\n",
    "process_ids = [res_dict['ID'] for res_dict in res_dict_list]\n",
    "\n",
    "cur_data_list = [item for item in data_list if item[\"ID\"] not in process_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 29,\n",
       " 'tweetFileID': 'data/124.txt',\n",
       " 'tweetFulltext': 'houstonflood ANYONE gets info on extent of buffalo bayou flooding near Allen Pkwy and Studemont St & Memorial Ct',\n",
       " 'tweetAnnotation': [{'locationDesc': 'buffalo bayou',\n",
       "   'locationCate': 'C6',\n",
       "   'startIndex': 43,\n",
       "   'endIndex': 56},\n",
       "  {'locationDesc': 'Allen Pkwy and Studemont St & Memorial Ct',\n",
       "   'locationCate': 'C11',\n",
       "   'startIndex': 71,\n",
       "   'endIndex': 112}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_data_list[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"\"   ## note: An OpenAI API key needs to be added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = get_place_recognition_answer_logits(cur_data_list[0], recog_prompt, tokenizer, \n",
    "                                                   text_key = \"tweetFulltext\",\n",
    "                                                    engine = engine, \n",
    "                                                    n_tokens = n_tokens, \n",
    "                                                    temp=temp, \n",
    "                                                    logprobs = logprobs)\n",
    "\n",
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' CX: Houston; CX: i-45 & n. main street'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cur_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:07:49<00:00,  4.07s/it]\n"
     ]
    }
   ],
   "source": [
    "#count = 0\n",
    "for i, row in (pbar := tqdm(enumerate(cur_data_list), total= len(cur_data_list) )):\n",
    "    res_dict = get_place_recognition_answer_logits(row, recog_prompt, tokenizer, \n",
    "                                                   text_key = \"tweetFulltext\",\n",
    "                                                    engine = engine, \n",
    "                                                    n_tokens = n_tokens, \n",
    "                                                    temp=temp, \n",
    "                                                    logprobs = logprobs)\n",
    "    \n",
    "    with open(res_file, 'a') as f:\n",
    "        f.write(json.dumps(res_dict) + \"\\n\")\n",
    "        \n",
    "    # print the result for a preview\n",
    "#     print(\"Tweet ID is: \"+ res_dict['tweetFileID'])\n",
    "#     print(\"Tweet text is: \"+ res_dict['tweetFulltext'])\n",
    "#     print(\"Annotation is: \"+ str(res_dict['tweetAnnotation']))\n",
    "#     print(\"Prediction is: \"+ res_dict['answer'])\n",
    "    \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # Test only retrieve 10\n",
    "#     count +=1\n",
    "#     if count == 11:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GPT-3 without geo-knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "recog_prompt = \"\"\"\n",
    "This is a set of location description recognition problems. \n",
    "The `Sentence` is a sentence containing location descriptions. The goal is to infer which parts of the sentence represent location descriptions and put them in double quotes.\n",
    "--\n",
    "\n",
    "--\n",
    "Sentence: {TEXT}\n",
    "Q: Which parts of this sentence represent location descriptions?\n",
    "A:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/\"\n",
    "anno_file = os.path.join(data_dir, 'tweetTextAndAnnotation.txt')\n",
    "data_list = load_HarveyTweet2017_data(anno_file, data_dir, sep = \"||\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine = \"text-davinci-002\"  \n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "n_tokens = 30\n",
    "temp=0.2\n",
    "logprobs = 5\n",
    "logprobs_topk = 20\n",
    "\n",
    "res_dir = \"result/\"  \n",
    "if not os.path.exists(res_dir):\n",
    "    os.mkdir(res_dir)\n",
    "res_file = \"{res_dir}/placerecognize_{engine}_{temp:.3f}_{n_tokens:d}_{logprobs:d}_no_geo.json\".format(\n",
    "        res_dir = res_dir,\n",
    "        engine = engine,\n",
    "        temp = temp,\n",
    "        n_tokens = n_tokens,\n",
    "        logprobs = logprobs\n",
    "    )\n",
    "res_dict_list = read_res_file(res_file)\n",
    "process_ids = [res_dict['ID'] for res_dict in res_dict_list]\n",
    "\n",
    "cur_data_list = [item for item in data_list if item[\"ID\"] not in process_ids]\n",
    "len(cur_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 988/988 [1:14:26<00:00,  4.52s/it]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i, row in (pbar := tqdm(enumerate(cur_data_list), total= len(cur_data_list) )):\n",
    "    res_dict = get_place_recognition_answer_logits(row, recog_prompt, tokenizer, \n",
    "                                                   text_key = \"tweetFulltext\",\n",
    "                                                    engine = engine, \n",
    "                                                    n_tokens = n_tokens, \n",
    "                                                    temp=temp, \n",
    "                                                    logprobs = logprobs)\n",
    "    \n",
    "    original_answer = res_dict['answer']\n",
    "    extracted_answer = re.findall('\"([^\"]*)\"', original_answer)\n",
    "    \n",
    "    res_dict['origin_answer'] = res_dict['answer'] \n",
    "    res_dict['answer'] = \";\".join([\"CX: \"+str(this_str) for this_str in extracted_answer])\n",
    "    \n",
    "    \n",
    "    with open(res_file, 'a') as f:\n",
    "        f.write(json.dumps(res_dict) + \"\\n\")\n",
    "        \n",
    "    # print the result for a preview\n",
    "#     print(\"Tweet ID is: \"+ res_dict['tweetFileID'])\n",
    "#     print(\"Tweet text is: \"+ res_dict['tweetFulltext'])\n",
    "#     print(\"Annotation is: \"+ str(res_dict['tweetAnnotation']))\n",
    "#     print(\"Original answer is: \"+ res_dict['origin_answer'])\n",
    "#     print(\"Answer is: \"+ res_dict['answer'])\n",
    "    \n",
    "    count += 1\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "#     if count == 2:\n",
    "#         break\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
